{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from requests import status_codes\n",
    "\n",
    "from pytrends import exceptions\n",
    "\n",
    "from urllib.parse import quote\n",
    "\n",
    "\n",
    "from datetime import date\n",
    "from pytrends.exceptions import ResponseError\n",
    "import time\n",
    "import numpy as np\n",
    "from random import randint\n",
    "import os\n",
    "\n",
    "BASE_TRENDS_URL = 'https://trends.google.com/trends'\n",
    "\n",
    "\n",
    "class TrendReq(object):\n",
    "    \"\"\"\n",
    "    Google Trends API\n",
    "    \"\"\"\n",
    "    GET_METHOD = 'get'\n",
    "    POST_METHOD = 'post'\n",
    "    GENERAL_URL = f'{BASE_TRENDS_URL}/api/explore'\n",
    "    INTEREST_OVER_TIME_URL = f'{BASE_TRENDS_URL}/api/widgetdata/multiline'\n",
    "    MULTIRANGE_INTEREST_OVER_TIME_URL = f'{BASE_TRENDS_URL}/api/widgetdata/multirange'\n",
    "    INTEREST_BY_REGION_URL = f'{BASE_TRENDS_URL}/api/widgetdata/comparedgeo'\n",
    "    RELATED_QUERIES_URL = f'{BASE_TRENDS_URL}/api/widgetdata/relatedsearches'\n",
    "    TRENDING_SEARCHES_URL = f'{BASE_TRENDS_URL}/hottrends/visualize/internal/data'\n",
    "    TOP_CHARTS_URL = f'{BASE_TRENDS_URL}/api/topcharts'\n",
    "    SUGGESTIONS_URL = f'{BASE_TRENDS_URL}/api/autocomplete/'\n",
    "    CATEGORIES_URL = f'{BASE_TRENDS_URL}/api/explore/pickers/category'\n",
    "    TODAY_SEARCHES_URL = f'{BASE_TRENDS_URL}/api/dailytrends'\n",
    "    REALTIME_TRENDING_SEARCHES_URL = f'{BASE_TRENDS_URL}/api/realtimetrends'\n",
    "    ERROR_CODES = (500, 502, 504, 429)\n",
    "\n",
    "    def __init__(self, hl='en-US', tz=360, geo='', timeout=(2, 5), proxies='',\n",
    "                 retries=0, backoff_factor=0, requests_args=None):\n",
    "        \"\"\"\n",
    "        Initialize default values for params\n",
    "        \"\"\"\n",
    "        # google rate limit\n",
    "        self.google_rl = 'You have reached your quota limit. Please try again later.'\n",
    "        self.results = None\n",
    "        # set user defined options used globally\n",
    "        self.tz = tz\n",
    "        self.hl = hl\n",
    "        self.geo = geo\n",
    "        self.kw_list = list()\n",
    "        self.timeout = timeout\n",
    "        self.proxies = proxies  # add a proxy option\n",
    "        self.retries = retries\n",
    "        self.backoff_factor = backoff_factor\n",
    "        self.proxy_index = 0\n",
    "        self.requests_args = requests_args or {}\n",
    "        self.cookies = self.GetGoogleCookie()\n",
    "        # intialize widget payloads\n",
    "        self.token_payload = dict()\n",
    "        self.interest_over_time_widget = dict()\n",
    "        self.interest_by_region_widget = dict()\n",
    "        self.related_topics_widget_list = list()\n",
    "        self.related_queries_widget_list = list()\n",
    "\n",
    "        self.headers = {'accept-language': self.hl}\n",
    "        self.headers.update(self.requests_args.pop('headers', {}))\n",
    "        \n",
    "    def GetGoogleCookie(self):\n",
    "        \"\"\"\n",
    "        Gets google cookie (used for each and every proxy; once on init otherwise)\n",
    "        Removes proxy from the list on proxy error\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            if \"proxies\" in self.requests_args:\n",
    "                try:\n",
    "                    return dict(filter(lambda i: i[0] == 'NID', requests.post(\n",
    "                        f'{BASE_TRENDS_URL}/?geo={self.hl[-2:]}',\n",
    "                        timeout=self.timeout,\n",
    "                        **self.requests_args\n",
    "                    ).cookies.items()))\n",
    "                except:\n",
    "                    continue\n",
    "            else:\n",
    "                if len(self.proxies) > 0:\n",
    "                    proxy = {'https': self.proxies[self.proxy_index]}\n",
    "                else:\n",
    "                    proxy = ''\n",
    "                try:\n",
    "                    return dict(filter(lambda i: i[0] == 'NID', requests.post(\n",
    "                        f'{BASE_TRENDS_URL}/?geo={self.hl[-2:]}',\n",
    "                        timeout=self.timeout,\n",
    "                        proxies=proxy,\n",
    "                        **self.requests_args\n",
    "                    ).cookies.items()))\n",
    "                except requests.exceptions.ProxyError:\n",
    "                    print('Proxy error. Changing IP')\n",
    "                    if len(self.proxies) > 1:\n",
    "                        self.proxies.remove(self.proxies[self.proxy_index])\n",
    "                    else:\n",
    "                        print('No more proxies available. Bye!')\n",
    "                        raise\n",
    "                    continue\n",
    "\n",
    "    def GetNewProxy(self):\n",
    "        \"\"\"\n",
    "        Increment proxy INDEX; zero on overflow\n",
    "        \"\"\"\n",
    "        if self.proxy_index < (len(self.proxies) - 1):\n",
    "            self.proxy_index += 1\n",
    "        else:\n",
    "            self.proxy_index = 0\n",
    "\n",
    "    def _get_data(self, url, method=GET_METHOD, trim_chars=0, **kwargs):\n",
    "        \"\"\"Send a request to Google and return the JSON response as a Python object\n",
    "        :param url: the url to which the request will be sent\n",
    "        :param method: the HTTP method ('get' or 'post')\n",
    "        :param trim_chars: how many characters should be trimmed off the beginning of the content of the response\n",
    "            before this is passed to the JSON parser\n",
    "        :param kwargs: any extra key arguments passed to the request builder (usually query parameters or data)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        s = requests.session()\n",
    "        # Retries mechanism. Activated when one of statements >0 (best used for proxy)\n",
    "        if self.retries > 0 or self.backoff_factor > 0:\n",
    "            retry = Retry(total=self.retries, read=self.retries,\n",
    "                          connect=self.retries,\n",
    "                          backoff_factor=self.backoff_factor,\n",
    "                          status_forcelist=TrendReq.ERROR_CODES,\n",
    "                          allowed_methods=frozenset(['GET', 'POST']))\n",
    "            s.mount('https://', HTTPAdapter(max_retries=retry))\n",
    "\n",
    "        s.headers.update(self.headers)\n",
    "        if len(self.proxies) > 0:\n",
    "            self.cookies = self.GetGoogleCookie()\n",
    "            s.proxies.update({'https': self.proxies[self.proxy_index]})\n",
    "        if method == TrendReq.POST_METHOD:\n",
    "            response = s.post(url, timeout=self.timeout,\n",
    "                              cookies=self.cookies, **kwargs,\n",
    "                              **self.requests_args)  # DO NOT USE retries or backoff_factor here\n",
    "        else:\n",
    "            response = s.get(url, timeout=self.timeout, cookies=self.cookies,\n",
    "                             **kwargs, **self.requests_args)  # DO NOT USE retries or backoff_factor here\n",
    "        # check if the response contains json and throw an exception otherwise\n",
    "        # Google mostly sends 'application/json' in the Content-Type header,\n",
    "        # but occasionally it sends 'application/javascript\n",
    "        # and sometimes even 'text/javascript\n",
    "        if response.status_code == 200 and 'application/json' in \\\n",
    "                response.headers['Content-Type'] or \\\n",
    "                'application/javascript' in response.headers['Content-Type'] or \\\n",
    "                'text/javascript' in response.headers['Content-Type']:\n",
    "            # trim initial characters\n",
    "            # some responses start with garbage characters, like \")]}',\"\n",
    "            # these have to be cleaned before being passed to the json parser\n",
    "            content = response.text[trim_chars:]\n",
    "            # parse json\n",
    "            self.GetNewProxy()\n",
    "            return json.loads(content)\n",
    "        else:\n",
    "            if response.status_code == status_codes.codes.too_many_requests:\n",
    "                raise exceptions.TooManyRequestsError.from_response(response)\n",
    "            raise exceptions.ResponseError.from_response(response)\n",
    "\n",
    "    def build_payload(self, kw_list, cat=0, timeframe='today 5-y', geo='',\n",
    "                      gprop=''):\n",
    "        \"\"\"Create the payload for related queries, interest over time and interest by region\"\"\"\n",
    "        if gprop not in ['', 'images', 'news', 'youtube', 'froogle']:\n",
    "            raise ValueError('gprop must be empty (to indicate web), images, news, youtube, or froogle')\n",
    "        self.kw_list = kw_list\n",
    "        self.geo = geo or self.geo\n",
    "        self.token_payload = {\n",
    "            'hl': self.hl,\n",
    "            'tz': self.tz,\n",
    "            'req': {'comparisonItem': [], 'category': cat, 'property': gprop}\n",
    "        }\n",
    "\n",
    "        # Check if timeframe is a list\n",
    "        if isinstance(timeframe, list):\n",
    "            for index, kw in enumerate(self.kw_list):\n",
    "                keyword_payload = {'keyword': kw, 'time': timeframe[index], 'geo': self.geo}\n",
    "                self.token_payload['req']['comparisonItem'].append(keyword_payload)\n",
    "        else: \n",
    "            # build out json for each keyword with\n",
    "            for kw in self.kw_list:\n",
    "                keyword_payload = {'keyword': kw, 'time': timeframe, 'geo': self.geo}\n",
    "                self.token_payload['req']['comparisonItem'].append(keyword_payload)\n",
    "\n",
    "        # requests will mangle this if it is not a string\n",
    "        self.token_payload['req'] = json.dumps(self.token_payload['req'])\n",
    "        # get tokens\n",
    "        self._tokens()\n",
    "        return\n",
    "\n",
    "    def _tokens(self):\n",
    "        \"\"\"Makes request to Google to get API tokens for interest over time, interest by region and related queries\"\"\"\n",
    "        # make the request and parse the returned json\n",
    "        widget_dicts = self._get_data(\n",
    "            url=TrendReq.GENERAL_URL,\n",
    "            method=TrendReq.GET_METHOD,\n",
    "            params=self.token_payload,\n",
    "            trim_chars=4,\n",
    "        )['widgets']\n",
    "        # order of the json matters...\n",
    "        first_region_token = True\n",
    "        # clear self.related_queries_widget_list and self.related_topics_widget_list\n",
    "        # of old keywords'widgets\n",
    "        self.related_queries_widget_list[:] = []\n",
    "        self.related_topics_widget_list[:] = []\n",
    "        # assign requests\n",
    "        for widget in widget_dicts:\n",
    "            if widget['id'] == 'TIMESERIES':\n",
    "                self.interest_over_time_widget = widget\n",
    "            if widget['id'] == 'GEO_MAP' and first_region_token:\n",
    "                self.interest_by_region_widget = widget\n",
    "                first_region_token = False\n",
    "            # response for each term, put into a list\n",
    "            if 'RELATED_TOPICS' in widget['id']:\n",
    "                self.related_topics_widget_list.append(widget)\n",
    "            if 'RELATED_QUERIES' in widget['id']:\n",
    "                self.related_queries_widget_list.append(widget)\n",
    "        return\n",
    "\n",
    "    def interest_over_time(self):\n",
    "        \"\"\"Request data from Google's Interest Over Time section and return a dataframe\"\"\"\n",
    "\n",
    "        over_time_payload = {\n",
    "            # convert to string as requests will mangle\n",
    "            'req': json.dumps(self.interest_over_time_widget['request']),\n",
    "            'token': self.interest_over_time_widget['token'],\n",
    "            'tz': self.tz\n",
    "        }\n",
    "\n",
    "        # make the request and parse the returned json\n",
    "        req_json = self._get_data(\n",
    "            url=TrendReq.INTEREST_OVER_TIME_URL,\n",
    "            method=TrendReq.GET_METHOD,\n",
    "            trim_chars=5,\n",
    "            params=over_time_payload,\n",
    "        )\n",
    "\n",
    "        df = pd.DataFrame(req_json['default']['timelineData'])\n",
    "        if (df.empty):\n",
    "            return df\n",
    "\n",
    "        df['date'] = pd.to_datetime(df['time'].astype(dtype='float64'),\n",
    "                                    unit='s')\n",
    "        df = df.set_index(['date']).sort_index()\n",
    "        # split list columns into seperate ones, remove brackets and split on comma\n",
    "        result_df = df['value'].apply(lambda x: pd.Series(\n",
    "            str(x).replace('[', '').replace(']', '').split(',')))\n",
    "        # rename each column with its search term, relying on order that google provides...\n",
    "        for idx, kw in enumerate(self.kw_list):\n",
    "            # there is currently a bug with assigning columns that may be\n",
    "            # parsed as a date in pandas: use explicit insert column method\n",
    "            result_df.insert(len(result_df.columns), kw,\n",
    "                             result_df[idx].astype('int'))\n",
    "            del result_df[idx]\n",
    "\n",
    "        if 'isPartial' in df:\n",
    "            # make other dataframe from isPartial key data\n",
    "            # split list columns into seperate ones, remove brackets and split on comma\n",
    "            df = df.fillna(False)\n",
    "            result_df2 = df['isPartial'].apply(lambda x: pd.Series(\n",
    "                str(x).replace('[', '').replace(']', '').split(',')))\n",
    "            result_df2.columns = ['isPartial']\n",
    "            # Change to a bool type.\n",
    "            result_df2.isPartial = result_df2.isPartial == 'True'\n",
    "            # concatenate the two dataframes\n",
    "            final = pd.concat([result_df, result_df2], axis=1)\n",
    "        else:\n",
    "            final = result_df\n",
    "            final['isPartial'] = False\n",
    "\n",
    "        return final\n",
    "\n",
    "    def multirange_interest_over_time(self):\n",
    "        \"\"\"Request data from Google's Interest Over Time section across different time ranges and return a dataframe\"\"\"\n",
    "\n",
    "        over_time_payload = {\n",
    "            # convert to string as requests will mangle\n",
    "            'req': json.dumps(self.interest_over_time_widget['request']),\n",
    "            'token': self.interest_over_time_widget['token'],\n",
    "            'tz': self.tz\n",
    "        }\n",
    "\n",
    "        # make the request and parse the returned json\n",
    "        req_json = self._get_data(\n",
    "            url=TrendReq.MULTIRANGE_INTEREST_OVER_TIME_URL,\n",
    "            method=TrendReq.GET_METHOD,\n",
    "            trim_chars=5,\n",
    "            params=over_time_payload,\n",
    "        )\n",
    "\n",
    "        df = pd.DataFrame(req_json['default']['timelineData'])\n",
    "        if (df.empty):\n",
    "            return df\n",
    "\n",
    "        result_df = pd.json_normalize(df['columnData'])\n",
    "\n",
    "        # Split dictionary columns into seperate ones\n",
    "        for i, column in enumerate(result_df.columns):\n",
    "            result_df[\"[\" + str(i) + \"] \" + str(self.kw_list[i]) + \" date\"] = result_df[i].apply(pd.Series)[\"formattedTime\"]\n",
    "            result_df[\"[\" + str(i) + \"] \" + str(self.kw_list[i]) + \" value\"] = result_df[i].apply(pd.Series)[\"value\"]   \n",
    "            result_df = result_df.drop([i], axis=1)\n",
    "        \n",
    "        # Adds a row with the averages at the top of the dataframe\n",
    "        avg_row = {}\n",
    "        for i, avg in enumerate(req_json['default']['averages']):\n",
    "            avg_row[\"[\" + str(i) + \"] \" + str(self.kw_list[i]) + \" date\"] = \"Average\"\n",
    "            avg_row[\"[\" + str(i) + \"] \" + str(self.kw_list[i]) + \" value\"] = req_json['default']['averages'][i]\n",
    "\n",
    "        result_df.loc[-1] = avg_row\n",
    "        result_df.index = result_df.index + 1\n",
    "        result_df = result_df.sort_index()\n",
    "        \n",
    "        return result_df\n",
    "\n",
    "\n",
    "    def interest_by_region(self, resolution='COUNTRY', inc_low_vol=False,\n",
    "                           inc_geo_code=False):\n",
    "        \"\"\"Request data from Google's Interest by Region section and return a dataframe\"\"\"\n",
    "\n",
    "        # make the request\n",
    "        region_payload = dict()\n",
    "        if self.geo == '':\n",
    "            self.interest_by_region_widget['request'][\n",
    "                'resolution'] = resolution\n",
    "        elif self.geo == 'US' and resolution in ['DMA', 'CITY', 'REGION']:\n",
    "            self.interest_by_region_widget['request'][\n",
    "                'resolution'] = resolution\n",
    "\n",
    "        self.interest_by_region_widget['request'][\n",
    "            'includeLowSearchVolumeGeos'] = inc_low_vol\n",
    "\n",
    "        # convert to string as requests will mangle\n",
    "        region_payload['req'] = json.dumps(\n",
    "            self.interest_by_region_widget['request'])\n",
    "        region_payload['token'] = self.interest_by_region_widget['token']\n",
    "        region_payload['tz'] = self.tz\n",
    "\n",
    "        # parse returned json\n",
    "        req_json = self._get_data(\n",
    "            url=TrendReq.INTEREST_BY_REGION_URL,\n",
    "            method=TrendReq.GET_METHOD,\n",
    "            trim_chars=5,\n",
    "            params=region_payload,\n",
    "        )\n",
    "        df = pd.DataFrame(req_json['default']['geoMapData'])\n",
    "        if (df.empty):\n",
    "            return df\n",
    "\n",
    "        # rename the column with the search keyword\n",
    "        geo_column = 'geoCode' if 'geoCode' in df.columns else 'coordinates'\n",
    "        columns = ['geoName', geo_column, 'value']\n",
    "        df = df[columns].set_index(['geoName']).sort_index()\n",
    "        # split list columns into separate ones, remove brackets and split on comma\n",
    "        result_df = df['value'].apply(lambda x: pd.Series(\n",
    "            str(x).replace('[', '').replace(']', '').split(',')))\n",
    "        if inc_geo_code:\n",
    "            if geo_column in df.columns:\n",
    "                result_df[geo_column] = df[geo_column]\n",
    "            else:\n",
    "                print('Could not find geo_code column; Skipping')\n",
    "\n",
    "        # rename each column with its search term\n",
    "        for idx, kw in enumerate(self.kw_list):\n",
    "            result_df[kw] = result_df[idx].astype('int')\n",
    "            del result_df[idx]\n",
    "\n",
    "        return result_df\n",
    "\n",
    "    def related_topics(self):\n",
    "        \"\"\"Request data from Google's Related Topics section and return a dictionary of dataframes\n",
    "        If no top and/or rising related topics are found, the value for the key \"top\" and/or \"rising\" will be None\n",
    "        \"\"\"\n",
    "\n",
    "        # make the request\n",
    "        related_payload = dict()\n",
    "        result_dict = dict()\n",
    "        for request_json in self.related_topics_widget_list:\n",
    "            # ensure we know which keyword we are looking at rather than relying on order\n",
    "            try:\n",
    "                kw = request_json['request']['restriction'][\n",
    "                    'complexKeywordsRestriction']['keyword'][0]['value']\n",
    "            except KeyError:\n",
    "                kw = ''\n",
    "            # convert to string as requests will mangle\n",
    "            related_payload['req'] = json.dumps(request_json['request'])\n",
    "            related_payload['token'] = request_json['token']\n",
    "            related_payload['tz'] = self.tz\n",
    "\n",
    "            # parse the returned json\n",
    "            req_json = self._get_data(\n",
    "                url=TrendReq.RELATED_QUERIES_URL,\n",
    "                method=TrendReq.GET_METHOD,\n",
    "                trim_chars=5,\n",
    "                params=related_payload,\n",
    "            )\n",
    "\n",
    "            # top topics\n",
    "            try:\n",
    "                top_list = req_json['default']['rankedList'][0]['rankedKeyword']\n",
    "                df_top = pd.json_normalize(top_list, sep='_')\n",
    "            except KeyError:\n",
    "                # in case no top topics are found, the lines above will throw a KeyError\n",
    "                df_top = None\n",
    "\n",
    "            # rising topics\n",
    "            try:\n",
    "                rising_list = req_json['default']['rankedList'][1]['rankedKeyword']\n",
    "                df_rising = pd.json_normalize(rising_list, sep='_')\n",
    "            except KeyError:\n",
    "                # in case no rising topics are found, the lines above will throw a KeyError\n",
    "                df_rising = None\n",
    "\n",
    "            result_dict[kw] = {'rising': df_rising, 'top': df_top}\n",
    "        return result_dict\n",
    "\n",
    "    def related_queries(self):\n",
    "        \"\"\"Request data from Google's Related Queries section and return a dictionary of dataframes\n",
    "        If no top and/or rising related queries are found, the value for the key \"top\" and/or \"rising\" will be None\n",
    "        \"\"\"\n",
    "\n",
    "        # make the request\n",
    "        related_payload = dict()\n",
    "        result_dict = dict()\n",
    "        for request_json in self.related_queries_widget_list:\n",
    "            # ensure we know which keyword we are looking at rather than relying on order\n",
    "            try:\n",
    "                kw = request_json['request']['restriction'][\n",
    "                    'complexKeywordsRestriction']['keyword'][0]['value']\n",
    "            except KeyError:\n",
    "                kw = ''\n",
    "            # convert to string as requests will mangle\n",
    "            related_payload['req'] = json.dumps(request_json['request'])\n",
    "            related_payload['token'] = request_json['token']\n",
    "            related_payload['tz'] = self.tz\n",
    "\n",
    "            # parse the returned json\n",
    "            req_json = self._get_data(\n",
    "                url=TrendReq.RELATED_QUERIES_URL,\n",
    "                method=TrendReq.GET_METHOD,\n",
    "                trim_chars=5,\n",
    "                params=related_payload,\n",
    "            )\n",
    "\n",
    "            # top queries\n",
    "            try:\n",
    "                top_df = pd.DataFrame(\n",
    "                    req_json['default']['rankedList'][0]['rankedKeyword'])\n",
    "                top_df = top_df[['query', 'value']]\n",
    "            except KeyError:\n",
    "                # in case no top queries are found, the lines above will throw a KeyError\n",
    "                top_df = None\n",
    "\n",
    "            # rising queries\n",
    "            try:\n",
    "                rising_df = pd.DataFrame(\n",
    "                    req_json['default']['rankedList'][1]['rankedKeyword'])\n",
    "                rising_df = rising_df[['query', 'value']]\n",
    "            except KeyError:\n",
    "                # in case no rising queries are found, the lines above will throw a KeyError\n",
    "                rising_df = None\n",
    "\n",
    "            result_dict[kw] = {'top': top_df, 'rising': rising_df}\n",
    "        return result_dict\n",
    "\n",
    "    def trending_searches(self, pn='united_states'):\n",
    "        \"\"\"Request data from Google's Hot Searches section and return a dataframe\"\"\"\n",
    "\n",
    "        # make the request\n",
    "        # forms become obsolete due to the new TRENDING_SEARCHES_URL\n",
    "        # forms = {'ajax': 1, 'pn': pn, 'htd': '', 'htv': 'l'}\n",
    "        req_json = self._get_data(\n",
    "            url=TrendReq.TRENDING_SEARCHES_URL,\n",
    "            method=TrendReq.GET_METHOD\n",
    "        )[pn]\n",
    "        result_df = pd.DataFrame(req_json)\n",
    "        return result_df\n",
    "\n",
    "    def today_searches(self, pn='US'):\n",
    "        \"\"\"Request data from Google Daily Trends section and returns a dataframe\"\"\"\n",
    "        forms = {'ns': 15, 'geo': pn, 'tz': '-180', 'hl': 'en-US'}\n",
    "        req_json = self._get_data(\n",
    "            url=TrendReq.TODAY_SEARCHES_URL,\n",
    "            method=TrendReq.GET_METHOD,\n",
    "            trim_chars=5,\n",
    "            params=forms,\n",
    "            **self.requests_args\n",
    "        )['default']['trendingSearchesDays'][0]['trendingSearches']\n",
    "        # parse the returned json\n",
    "        result_df = pd.DataFrame(trend['title'] for trend in req_json)\n",
    "        return result_df.iloc[:, -1]\n",
    "\n",
    "    def realtime_trending_searches(self, pn='US', cat='all', count =300):\n",
    "        \"\"\"Request data from Google Realtime Search Trends section and returns a dataframe\"\"\"\n",
    "        # Don't know what some of the params mean here, followed the nodejs library\n",
    "        # https://github.com/pat310/google-trends-api/ 's implemenration\n",
    "\n",
    "\n",
    "        #sort: api accepts only 0 as the value, optional parameter\n",
    "\n",
    "        # ri: number of trending stories IDs returned,\n",
    "        # max value of ri supported is 300, based on emperical evidence\n",
    "\n",
    "        ri_value = 300\n",
    "        if count < ri_value:\n",
    "            ri_value = count\n",
    "\n",
    "        # rs : don't know what is does but it's max value is never more than the ri_value based on emperical evidence\n",
    "        # max value of ri supported is 200, based on emperical evidence\n",
    "        rs_value = 200\n",
    "        if count < rs_value:\n",
    "            rs_value = count-1\n",
    "\n",
    "        forms = {'ns': 15, 'geo': pn, 'tz': '300', 'hl': 'en-US', 'cat': cat, 'fi' : '0', 'fs' : '0', 'ri' : ri_value, 'rs' : rs_value, 'sort' : 0}\n",
    "        req_json = self._get_data(\n",
    "            url=TrendReq.REALTIME_TRENDING_SEARCHES_URL,\n",
    "            method=TrendReq.GET_METHOD,\n",
    "            trim_chars=5,\n",
    "            params=forms\n",
    "        )['storySummaries']['trendingStories']\n",
    "\n",
    "        # parse the returned json\n",
    "        wanted_keys = [\"entityNames\", \"title\"]\n",
    "\n",
    "        final_json = [{ key: ts[key] for key in ts.keys() if key in wanted_keys} for ts in req_json ]\n",
    "\n",
    "        result_df = pd.DataFrame(final_json)\n",
    "\n",
    "        return result_df\n",
    "\n",
    "    def top_charts(self, date, hl='en-US', tz=300, geo='GLOBAL'):\n",
    "        \"\"\"Request data from Google's Top Charts section and return a dataframe\"\"\"\n",
    "\n",
    "        try:\n",
    "            date = int(date)\n",
    "        except:\n",
    "            raise ValueError(\n",
    "                'The date must be a year with format YYYY. See https://github.com/GeneralMills/pytrends/issues/355')\n",
    "\n",
    "        # create the payload\n",
    "        chart_payload = {'hl': hl, 'tz': tz, 'date': date, 'geo': geo,\n",
    "                         'isMobile': False}\n",
    "\n",
    "        # make the request and parse the returned json\n",
    "        req_json = self._get_data(\n",
    "            url=TrendReq.TOP_CHARTS_URL,\n",
    "            method=TrendReq.GET_METHOD,\n",
    "            trim_chars=5,\n",
    "            params=chart_payload\n",
    "        )\n",
    "        try:\n",
    "            df = pd.DataFrame(req_json['topCharts'][0]['listItems'])\n",
    "        except IndexError:\n",
    "            df = None\n",
    "        return df\n",
    "\n",
    "    def suggestions(self, keyword):\n",
    "        \"\"\"Request data from Google's Keyword Suggestion dropdown and return a dictionary\"\"\"\n",
    "\n",
    "        # make the request\n",
    "        kw_param = quote(keyword)\n",
    "        parameters = {'hl': self.hl}\n",
    "\n",
    "        req_json = self._get_data(\n",
    "            url=TrendReq.SUGGESTIONS_URL + kw_param,\n",
    "            params=parameters,\n",
    "            method=TrendReq.GET_METHOD,\n",
    "            trim_chars=5\n",
    "        )['default']['topics']\n",
    "        return req_json\n",
    "\n",
    "    def categories(self):\n",
    "        \"\"\"Request available categories data from Google's API and return a dictionary\"\"\"\n",
    "\n",
    "        params = {'hl': self.hl}\n",
    "\n",
    "        req_json = self._get_data(\n",
    "            url=TrendReq.CATEGORIES_URL,\n",
    "            params=params,\n",
    "            method=TrendReq.GET_METHOD,\n",
    "            trim_chars=5\n",
    "        )\n",
    "        return req_json\n",
    "\n",
    "    def get_historical_interest(self, *args, **kwargs):\n",
    "        raise NotImplementedError(\n",
    "            \"\"\"This method has been removed for incorrectness. It will be removed completely in v5.\n",
    "If you'd like similar functionality, please try implementing it yourself and consider submitting a pull request to add it to pytrends.\n",
    "          \n",
    "There is discussion at:\n",
    "https://github.com/GeneralMills/pytrends/pull/542\"\"\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-01\n",
      "2020-01-02\n",
      "2020-01-03\n",
      "2020-01-04\n",
      "2020-01-05\n",
      "2020-01-06\n",
      "2020-01-07\n",
      "2020-01-08\n",
      "2020-01-09\n",
      "2020-01-10\n",
      "2020-01-11\n",
      "2020-01-12\n",
      "2020-01-13\n",
      "2020-01-14\n",
      "2020-01-15\n",
      "2020-01-16\n",
      "2020-01-17\n",
      "2020-01-18\n",
      "2020-01-19\n",
      "2020-01-20\n",
      "2020-01-21\n",
      "2020-01-22\n",
      "2020-01-23\n",
      "2020-01-24\n",
      "2020-01-25\n",
      "2020-01-26\n",
      "2020-01-27\n",
      "2020-01-28\n",
      "2020-01-29\n",
      "2020-01-30\n",
      "2020-01-31\n",
      "2020-02-01\n",
      "2020-02-02\n",
      "2020-02-03\n",
      "2020-02-04\n",
      "2020-02-05\n",
      "2020-02-06\n",
      "2020-02-07\n",
      "2020-02-08\n",
      "2020-02-09\n",
      "2020-02-10\n",
      "2020-02-11\n",
      "2020-02-12\n",
      "2020-02-13\n",
      "2020-02-14\n",
      "2020-02-15\n",
      "2020-02-16\n",
      "2020-02-17\n",
      "2020-02-18\n",
      "2020-02-19\n",
      "2020-02-20\n",
      "2020-02-21\n",
      "2020-02-22\n",
      "2020-02-23\n",
      "2020-02-24\n",
      "2020-02-25\n",
      "2020-02-26\n",
      "2020-02-27\n",
      "2020-02-28\n",
      "2020-02-29\n",
      "2020-03-01\n",
      "2020-03-02\n",
      "2020-03-03\n",
      "2020-03-04\n",
      "2020-03-05\n",
      "2020-03-06\n",
      "2020-03-07\n",
      "2020-03-08\n",
      "2020-03-09\n",
      "2020-03-10\n",
      "2020-03-11\n",
      "2020-03-12\n",
      "2020-03-13\n",
      "2020-03-14\n",
      "2020-03-15\n",
      "2020-03-16\n",
      "2020-03-17\n",
      "2020-03-18\n",
      "2020-03-19\n",
      "2020-03-20\n",
      "2020-03-21\n",
      "2020-03-22\n",
      "2020-03-23\n",
      "2020-03-24\n",
      "2020-03-25\n",
      "2020-03-26\n",
      "2020-03-27\n",
      "2020-03-28\n",
      "2020-03-29\n",
      "2020-03-30\n",
      "2020-03-31\n",
      "2020-04-01\n",
      "2020-04-02\n",
      "2020-04-03\n",
      "2020-04-04\n",
      "2020-04-05\n",
      "2020-04-06\n",
      "2020-04-07\n",
      "2020-04-08\n",
      "2020-04-09\n",
      "2020-04-10\n",
      "2020-04-11\n",
      "2020-04-12\n",
      "2020-04-13\n",
      "2020-04-14\n",
      "2020-04-15\n",
      "2020-04-16\n",
      "2020-04-17\n",
      "2020-04-18\n",
      "2020-04-19\n",
      "2020-04-20\n",
      "2020-04-21\n",
      "2020-04-22\n",
      "2020-04-23\n",
      "2020-04-24\n",
      "2020-04-25\n",
      "2020-04-26\n",
      "2020-04-27\n",
      "2020-04-28\n",
      "2020-04-29\n",
      "2020-04-30\n",
      "2020-05-01\n",
      "2020-05-02\n",
      "2020-05-03\n",
      "2020-05-04\n",
      "2020-05-05\n",
      "2020-05-06\n",
      "2020-05-07\n",
      "2020-05-08\n",
      "2020-05-09\n",
      "2020-05-10\n",
      "2020-05-11\n",
      "2020-05-12\n",
      "2020-05-13\n",
      "2020-05-14\n",
      "2020-05-15\n",
      "2020-05-16\n",
      "2020-05-17\n",
      "2020-05-18\n",
      "2020-05-19\n",
      "2020-05-20\n",
      "2020-05-21\n",
      "2020-05-22\n",
      "2020-05-23\n",
      "2020-05-24\n",
      "2020-05-25\n",
      "2020-05-26\n",
      "2020-05-27\n",
      "2020-05-28\n",
      "2020-05-29\n",
      "2020-05-30\n",
      "2020-05-31\n",
      "2020-06-01\n",
      "2020-06-02\n",
      "2020-06-03\n",
      "2020-06-04\n",
      "2020-06-05\n",
      "2020-06-06\n",
      "2020-06-07\n",
      "2020-06-08\n",
      "2020-06-09\n",
      "2020-06-10\n",
      "2020-06-11\n",
      "2020-06-12\n",
      "2020-06-13\n",
      "2020-06-14\n",
      "2020-06-15\n",
      "2020-06-16\n",
      "2020-06-17\n",
      "2020-06-18\n",
      "2020-06-19\n",
      "2020-06-20\n",
      "2020-06-21\n",
      "2020-06-22\n",
      "2020-06-23\n",
      "2020-06-24\n",
      "2020-06-25\n",
      "2020-06-26\n",
      "2020-06-27\n",
      "2020-06-28\n",
      "2020-06-29\n",
      "2020-06-30\n",
      "2020-07-01\n",
      "2020-07-02\n",
      "2020-07-03\n",
      "2020-07-04\n",
      "2020-07-05\n",
      "2020-07-06\n",
      "2020-07-07\n",
      "2020-07-08\n",
      "2020-07-09\n",
      "2020-07-10\n",
      "2020-07-11\n",
      "2020-07-12\n",
      "2020-07-13\n",
      "2020-07-14\n",
      "2020-07-15\n",
      "2020-07-16\n",
      "2020-07-17\n",
      "2020-07-18\n",
      "2020-07-19\n",
      "2020-07-20\n",
      "2020-07-21\n",
      "2020-07-22\n",
      "2020-07-23\n",
      "2020-07-24\n",
      "2020-07-25\n",
      "2020-07-26\n",
      "2020-07-27\n",
      "2020-07-28\n",
      "2020-07-29\n",
      "2020-07-30\n",
      "2020-07-31\n",
      "2020-08-01\n",
      "2020-08-02\n",
      "2020-08-03\n",
      "2020-08-04\n",
      "2020-08-05\n",
      "2020-08-06\n",
      "2020-08-07\n",
      "2020-08-08\n",
      "2020-08-09\n",
      "2020-08-10\n",
      "2020-08-11\n",
      "2020-08-12\n",
      "2020-08-13\n",
      "2020-08-14\n",
      "2020-08-15\n",
      "2020-08-16\n",
      "2020-08-17\n",
      "2020-08-18\n",
      "2020-08-19\n",
      "2020-08-20\n",
      "2020-08-21\n",
      "2020-08-22\n",
      "2020-08-23\n",
      "2020-08-24\n",
      "2020-08-25\n",
      "2020-08-26\n",
      "2020-08-27\n",
      "2020-08-28\n",
      "2020-08-29\n",
      "2020-08-30\n",
      "2020-08-31\n",
      "2020-09-01\n",
      "2020-09-02\n",
      "2020-09-03\n",
      "2020-09-04\n",
      "2020-09-05\n",
      "2020-09-06\n",
      "2020-09-07\n",
      "2020-09-08\n",
      "2020-09-09\n",
      "2020-09-10\n",
      "2020-09-11\n",
      "2020-09-12\n",
      "2020-09-13\n",
      "2020-09-14\n",
      "2020-09-15\n",
      "2020-09-16\n",
      "2020-09-17\n",
      "2020-09-18\n",
      "2020-09-19\n",
      "2020-09-20\n",
      "2020-09-21\n",
      "2020-09-22\n",
      "2020-09-23\n",
      "2020-09-24\n",
      "2020-09-25\n",
      "2020-09-26\n",
      "2020-09-27\n",
      "2020-09-28\n",
      "2020-09-29\n",
      "2020-09-30\n",
      "2020-10-01\n",
      "2020-10-02\n",
      "2020-10-03\n",
      "2020-10-04\n",
      "2020-10-05\n",
      "2020-10-06\n",
      "2020-10-07\n",
      "2020-10-08\n",
      "2020-10-09\n",
      "2020-10-10\n",
      "2020-10-11\n",
      "2020-10-12\n",
      "2020-10-13\n",
      "2020-10-14\n",
      "2020-10-15\n",
      "2020-10-16\n",
      "2020-10-17\n",
      "2020-10-18\n",
      "2020-10-19\n",
      "2020-10-20\n",
      "2020-10-21\n",
      "2020-10-22\n",
      "2020-10-23\n",
      "2020-10-24\n",
      "2020-10-25\n",
      "2020-10-26\n",
      "2020-10-27\n",
      "2020-10-28\n",
      "2020-10-29\n",
      "2020-10-30\n",
      "2020-10-31\n",
      "2020-11-01\n",
      "2020-11-02\n",
      "2020-11-03\n",
      "2020-11-04\n",
      "2020-11-05\n",
      "2020-11-06\n",
      "2020-11-07\n",
      "2020-11-08\n",
      "2020-11-09\n",
      "2020-11-10\n",
      "2020-11-11\n",
      "2020-11-12\n",
      "2020-11-13\n",
      "2020-11-14\n",
      "2020-11-15\n",
      "2020-11-16\n",
      "2020-11-17\n",
      "2020-11-18\n",
      "2020-11-19\n",
      "2020-11-20\n",
      "2020-11-21\n",
      "2020-11-22\n",
      "2020-11-23\n",
      "2020-11-24\n",
      "2020-11-25\n",
      "2020-11-26\n",
      "2020-11-27\n",
      "2020-11-28\n",
      "2020-11-29\n",
      "2020-11-30\n",
      "2020-12-01\n",
      "2020-12-02\n",
      "2020-12-03\n",
      "2020-12-04\n",
      "2020-12-05\n",
      "2020-12-06\n",
      "2020-12-07\n",
      "2020-12-08\n",
      "2020-12-09\n",
      "2020-12-10\n",
      "2020-12-11\n",
      "2020-12-12\n",
      "2020-12-13\n",
      "2020-12-14\n",
      "2020-12-15\n",
      "2020-12-16\n",
      "2020-12-17\n",
      "2020-12-18\n",
      "2020-12-19\n",
      "2020-12-20\n",
      "2020-12-21\n",
      "2020-12-22\n",
      "2020-12-23\n",
      "2020-12-24\n",
      "2020-12-25\n",
      "2020-12-26\n",
      "2020-12-27\n",
      "2020-12-28\n",
      "2020-12-29\n",
      "2020-12-30\n",
      "2020-12-31\n"
     ]
    }
   ],
   "source": [
    "start_date = '2020-01-01'\n",
    "end_date = '2020-12-31'\n",
    "geography=''\n",
    "\n",
    "\"\"\"\n",
    "Extracts the most trending topics on Google for a specified time range and saves the dataframe as json file\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "start_date_strip = ''.join(start_date.split('-'))\n",
    "end_date_strip = ''.join(end_date.split('-'))\n",
    "\n",
    "daily_trending_searches = pd.DataFrame()\n",
    "pytrend = TrendReq(hl='en-US',retries=3, backoff_factor=20)\n",
    "\n",
    "for i in [dr.strftime('%Y-%m-%d') for dr in pd.date_range(start_date_strip, end_date_strip)]:\n",
    "    try:\n",
    "        pytrend.build_payload(kw_list=[''], timeframe=i + ' ' + i, geo=geography)\n",
    "        df_rt_test = pytrend.related_topics()\n",
    "        data = df_rt_test['']['rising']\n",
    "        time.sleep(randint(11,15))\n",
    "        print(i)\n",
    "    except ResponseError:\n",
    "        print('Timeout')\n",
    "    daily_trending_searches = daily_trending_searches.append(data)\n",
    "\n",
    "\n",
    "daily_trending_searches.reset_index(drop=True, inplace=True)\n",
    "daily_trending_searches['date'] = daily_trending_searches['link'].apply(lambda x: x.split('&')[1].split('date=')[1].split('+')[0])\n",
    "daily_trending_searches['location'] = geography if geography != '' else 'World'\n",
    "location = geography if geography != '' else 'World' \n",
    "\n",
    "    # storing the data in JSON format\n",
    "    #daily_trending_searches.to_json(filepath + start_date + '_' + end_date + '_' + location + '_daily_trending_searches.json', orient = 'split', compression = 'infer', index = 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '../../data/interim/'\n",
    "daily_trending_searches.to_json(filepath + '2020_World_daily_trending_searches.json', orient = 'split', compression = 'infer', index = 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
