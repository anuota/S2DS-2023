{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-06 12:02:36.379833: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-06 12:02:41.240784: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-06 12:02:41.241025: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-06 12:02:41.241037: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-04-06 12:02:49.380934: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-04-06 12:02:49.382377: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-04-06 12:02:49.382424: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (a-18ira76222u8b): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "#Takes about 75 minutes to run on the whole dataset\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "import collections\n",
    "from rapidfuzz import process as pr\n",
    "import numpy as np\n",
    "from polyfuzz import PolyFuzz\n",
    "\n",
    "# Opening JSON file\n",
    "f = open('/home/ferdinand_t/data/CMS_2010_to_June_2022_ENGLISH.json')\n",
    "# returns JSON object as\n",
    "# a dictionary\n",
    "data = json.load(f)\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "#df['cleanLastModifiedDate'] = pd.to_datetime(df['lastModifiedDate'])\n",
    "#df.set_index('cleanLastModifiedDate', inplace = True)\n",
    "#df.sort_index().head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning step 1 out of 2 DONE. Number of unique keywords went from 156730 to 137404\n"
     ]
    }
   ],
   "source": [
    "lst_lst_keywords_clean = df['keywordStrings']\n",
    "\n",
    "    # Lower case\n",
    "lst_lst_keywords_clean = [list(map(str.casefold, x)) for x in lst_lst_keywords_clean]\n",
    "\n",
    "    # Split\n",
    "lst_lst_keywords_clean = [list(chain(*[kw.split(', ') for kw in lst_kw])) for lst_kw in lst_lst_keywords_clean]\n",
    "lst_lst_keywords_clean = [list(chain(*[kw.split(' - ') for kw in lst_kw])) for lst_kw in lst_lst_keywords_clean]\n",
    "\n",
    "    # Replace unicode and double spaces by a space\n",
    "lst_lst_keywords_clean = [list(map(lambda x: x.replace('\\xa0', ' '), lst_kw)) for lst_kw in lst_lst_keywords_clean]\n",
    "lst_lst_keywords_clean = [list(map(lambda x: x.replace('  ', ' '), lst_kw)) for lst_kw in lst_lst_keywords_clean]\n",
    "\n",
    "    # Replace unwanted characters\n",
    "lst_lst_keywords_clean = [list(map(lambda x: ''.join(filter(str.isprintable, x)), lst_kw)) for lst_kw in lst_lst_keywords_clean]\n",
    "lst_lst_keywords_clean = [list(map(lambda x: x.replace('.', ''), lst_kw)) for lst_kw in lst_lst_keywords_clean]\n",
    "lst_lst_keywords_clean = [list(map(lambda x: x.replace('\" ', ''), lst_kw)) for lst_kw in lst_lst_keywords_clean]\n",
    "lst_lst_keywords_clean = [list(map(lambda x: x.replace('\"', ''), lst_kw)) for lst_kw in lst_lst_keywords_clean]\n",
    "lst_lst_keywords_clean = [list(map(lambda x: x.replace(\"'\", ''), lst_kw)) for lst_kw in lst_lst_keywords_clean]\n",
    "lst_lst_keywords_clean = [list(map(lambda x: x.replace('keywords: ', ''), lst_kw)) for lst_kw in lst_lst_keywords_clean]\n",
    "\n",
    "    # Remove leading and trailing whitespaces\n",
    "lst_lst_keywords_clean = [list(map(lambda x: x.strip(), lst_kw)) for lst_kw in lst_lst_keywords_clean]\n",
    "\n",
    "    # Remove sentences (keywords that have more than 6 spaces)\n",
    "n_spaces = 6\n",
    "lst_lst_keywords_clean = [[kw for kw in lst_kw if kw.count(' ')<n_spaces] for lst_kw in lst_lst_keywords_clean]\n",
    "\n",
    "    # Outputs length of unique keywords before and after\n",
    "print('Cleaning step 1 out of 2 DONE. Number of unique keywords went from', len(set(list(chain(*df['keywordStrings'])))), \\\n",
    "    'to', len(set(list(chain(*lst_lst_keywords_clean)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_flat = list(chain(*lst_lst_keywords_clean)) # Flatten list\n",
    "keywords_freq = collections.Counter(keywords_flat)\n",
    "\n",
    "    # extract unique ones and remove the empty entry\n",
    "unique_keywords = list(set(keywords_flat))\n",
    "if '' in unique_keywords:\n",
    "    unique_keywords.remove('')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.97 should be a parameter, I have found that the lower it is the higher the run time\n",
    "from polyfuzz import PolyFuzz\n",
    "from polyfuzz.models import RapidFuzz\n",
    "\n",
    "model_df_matches = pd.DataFrame()\n",
    "rapidfuzz_matcher = RapidFuzz(n_jobs=1, score_cutoff = 0.97)\n",
    "#for i in range(100) :\n",
    "model = PolyFuzz(rapidfuzz_matcher).match(unique_keywords)\n",
    "model_df = model.get_matches()\n",
    "#model_df_matches.append(model_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(137403, 3)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df['Freq_From'] = pd.DataFrame([keywords_freq[word] for word in model_df['From']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df['Freq_To'] = pd.DataFrame([keywords_freq[word] for word in model_df['To']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df_96 = model_df[model_df['Similarity'] > 0.96]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleep = pd.DataFrame([model_df_96.iloc[i,model_df_96.columns.get_loc('From')] if model_df_96.iloc[i,model_df_96.columns.get_loc('Freq_From')] > model_df_96.iloc[i,model_df_96.columns.get_loc('Freq_To')] else model_df_96.iloc[i,model_df_96.columns.get_loc('To')] for i in range(model_df_96.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ferdinand_t/venv/lib64/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "model_df_96['replacement'] = bleep.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_flat = list(chain.from_iterable(lst_lst_keywords_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "haystack = keywords_flat\n",
    "needles = model_df_96['From']\n",
    "st = set(needles)\n",
    "index_96 = [i for i, e in enumerate(haystack) if e in st]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_flat_replace_2 = [model_df_96.iloc[list(model_df_96['From']).index(keywords_flat[i]),5] if keywords_flat[i] in model_df_96['From'].values else keywords_flat[i] for i in index_96]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (index, replacement) in zip(index_96, keywords_flat_replace_2):\n",
    "    keywords_flat[index] = replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning step 2 out of 2 DONE. Number of unique keywords went from 137404 to 135956\n"
     ]
    }
   ],
   "source": [
    "def gen_list_of_lists(original_list, new_structure):\n",
    "    assert len(original_list) == sum(new_structure), \\\n",
    "    \"The number of elements in the original list and desired structure don't match\"\n",
    "    list_of_lists = [[original_list[i + sum(new_structure[:j])] for i in range(new_structure[j])] \\\n",
    "                    for j in range(len(new_structure))]\n",
    "    return list_of_lists\n",
    "\n",
    "lst_lst_keywords_replaced = gen_list_of_lists(keywords_flat, [len(x) for x in lst_lst_keywords_clean])\n",
    "\n",
    "\n",
    "# Outputs length of unique keywords before and after\n",
    "print('Cleaning step 2 out of 2 DONE. Number of unique keywords went from', len(set(list(chain(*lst_lst_keywords_clean)))), \\\n",
    "    'to', len(set(list(chain(*lst_lst_keywords_replaced)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Needed to copy some functions to save the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_categories(df):\n",
    "    ''' \n",
    "    Cleans the category column of data frame df\n",
    "    1. Gets rid of the dictionary format\n",
    "    2. Extracts all the main (primary) categories\n",
    "\n",
    "    '''\n",
    "\n",
    "    # Makes a new column to get rid of the dictionary format\n",
    "    df['cleanFocusCategory'] = df['thematicFocusCategory'].apply(lambda x: x['name'] if x is not None else x)\n",
    "\n",
    "    # Convert all secondary categories into primary categories\n",
    "    children_dict = {'Architecture':'Culture', 'Design':'Culture', 'Film':'Culture', 'Arts':'Culture', \n",
    "                    'Literature':'Culture', 'Music':'Culture', 'Dance':'Culture', 'Theater':'Culture',\n",
    "                    'Climate':'Nature and Environment',\n",
    "                    'Conflicts':'Politics', 'Terrorism':'Politics', \n",
    "                    'Corruption':'Law and Justice', 'Crime':'Law and Justice', 'Rule of Law':'Law and Justice',\n",
    "                        'Press Freedom':'Law and Justice', \n",
    "                    'Diversity':'Human Rights', 'Freedom of Speech':'Human Rights', 'Equality':'Human Rights', \n",
    "                    'Soccer': 'Sports',\n",
    "                        'Trade':'Business', 'Globalization':'Business', 'Food Security':'Business'\n",
    "    }\n",
    "\n",
    "    secondary_cts = [val for val in children_dict.keys()]\n",
    "\n",
    "    # Replaces\n",
    "    df['cleanFocusParentCategory'] = df['cleanFocusCategory'].apply(lambda x: children_dict[x] if x in secondary_cts else x)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_categories(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = pd.to_datetime(df['lastModifiedDate']).apply(lambda x: x.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset_new = df[['id', 'lastModifiedDate', 'Date', 'keywordStrings', 'cleanFocusParentCategory', 'cleanFocusCategory', 'teaser']].copy()\n",
    "df_subset_new['keywordStringsCleanAfterFuzz'] = lst_lst_keywords_replaced\n",
    "    # Storing the data in JSON format\n",
    "filepath = '../../data/interim/clean_keywords_' + 'FULL' + '.json'\n",
    "df_subset_new.to_json(filepath, orient = 'split', compression = 'infer', index = 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
